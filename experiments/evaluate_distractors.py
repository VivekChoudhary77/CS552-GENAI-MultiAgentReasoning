"""Evaluation script for comparing distractor quality between baseline and multi-agent systems."""
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
from bert_score import score
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

logger = setup_logger()

class DistractorEvaluator:
    """Evaluates the quality of distractors using semantic similarity metrics."""
    
    def __init__(self, embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """Initialize the evaluator with an embedding model."""
        logger.info(f"Loading embedding model: {embedding_model_name}")
        self.embedder = SentenceTransformer(embedding_model_name)
    
    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """Calculate cosine similarity between two texts."""
        embeddings = self.embedder.encode([text1, text2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)
    
    def calculate_bertscore(self, correct_answer: str, distractor: str) -> Dict[str, float]:
        """Calculate BERTScore metrics for a distractor."""
        # BERTScore expects lists
        P, R, F1 = score([distractor], [correct_answer], lang='en', verbose=False)
        return {
            'precision': float(P[0].item()),
            'recall': float(R[0].item()),
            'f1': float(F1[0].item())
        }
    
    def evaluate_quiz(self, quiz_data: Dict) -> Dict:
        """
        Evaluate a single quiz's distractor quality.
        
        Args:
            quiz_data: Dictionary with 'correct_answer' and 'distractors' keys
            
        Returns:
            Dictionary with evaluation metrics
        """
        correct_answer = quiz_data['correct_answer']
        distractors = quiz_data['distractors']
        
        if not isinstance(distractors, list) or len(distractors) == 0:
            logger.warning("No distractors found in quiz data")
            return {}
        
        results = {
            'cosine_similarities': [],
            'bertscore_f1': [],
            'bertscore_precision': [],
            'bertscore_recall': [],
            'average_cosine_similarity': 0.0,
            'average_bertscore_f1': 0.0
        }
        
        for i, distractor in enumerate(distractors):
            # Cosine similarity
            cos_sim = self.calculate_cosine_similarity(correct_answer, distractor)
            results['cosine_similarities'].append(cos_sim)
            
            # BERTScore
            bert_metrics = self.calculate_bertscore(correct_answer, distractor)
            results['bertscore_f1'].append(bert_metrics['f1'])
            results['bertscore_precision'].append(bert_metrics['precision'])
            results['bertscore_recall'].append(bert_metrics['recall'])
            
            logger.debug(f"Distractor {i+1}: Cosine={cos_sim:.3f}, BERTScore F1={bert_metrics['f1']:.3f}")
        
        # Calculate averages
        results['average_cosine_similarity'] = np.mean(results['cosine_similarities'])
        results['average_bertscore_f1'] = np.mean(results['bertscore_f1'])
        
        return results
    
    def compare_systems(self, baseline_quiz: Dict, multiagent_quiz: Dict) -> Dict:
        """
        Compare baseline vs multi-agent system results.
        
        Args:
            baseline_quiz: Quiz generated by baseline system
            multiagent_quiz: Quiz generated by multi-agent system
            
        Returns:
            Comparison metrics
        """
        logger.info("Evaluating baseline quiz...")
        baseline_metrics = self.evaluate_quiz(baseline_quiz)
        
        logger.info("Evaluating multi-agent quiz...")
        multiagent_metrics = self.evaluate_quiz(multiagent_quiz)
        
        comparison = {
            'baseline': baseline_metrics,
            'multiagent': multiagent_metrics,
            'improvement': {
                'cosine_similarity': multiagent_metrics['average_cosine_similarity'] - baseline_metrics['average_cosine_similarity'],
                'bertscore_f1': multiagent_metrics['average_bertscore_f1'] - baseline_metrics['average_bertscore_f1']
            }
        }
        
        logger.info(f"Baseline avg cosine similarity: {baseline_metrics['average_cosine_similarity']:.3f}")
        logger.info(f"Multi-agent avg cosine similarity: {multiagent_metrics['average_cosine_similarity']:.3f}")
        logger.info(f"Improvement: {comparison['improvement']['cosine_similarity']:.3f}")
        
        return comparison

def main():
    """Main entry point for evaluation."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate distractor quality")
    parser.add_argument("--baseline", type=str, required=True, help="Path to baseline quiz JSON file")
    parser.add_argument("--multiagent", type=str, required=True, help="Path to multi-agent quiz JSON file")
    parser.add_argument("--output", type=str, default="evaluation_results.json", help="Output file for results")
    
    args = parser.parse_args()
    
    # Load quiz files
    try:
        with open(args.baseline, 'r') as f:
            baseline_quiz = json.load(f)
        with open(args.multiagent, 'r') as f:
            multiagent_quiz = json.load(f)
    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        return
    except json.JSONDecodeError as e:
        logger.error(f"Error parsing JSON: {e}")
        return
    
    # Evaluate
    evaluator = DistractorEvaluator()
    comparison = evaluator.compare_systems(baseline_quiz, multiagent_quiz)
    
    # Print results
    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)
    print(f"\nBaseline System:")
    print(f"  Average Cosine Similarity: {comparison['baseline']['average_cosine_similarity']:.3f}")
    print(f"  Average BERTScore F1: {comparison['baseline']['average_bertscore_f1']:.3f}")
    
    print(f"\nMulti-Agent System:")
    print(f"  Average Cosine Similarity: {comparison['multiagent']['average_cosine_similarity']:.3f}")
    print(f"  Average BERTScore F1: {comparison['multiagent']['average_bertscore_f1']:.3f}")
    
    print(f"\nImprovement:")
    print(f"  Cosine Similarity: {comparison['improvement']['cosine_similarity']:+.3f}")
    print(f"  BERTScore F1: {comparison['improvement']['bertscore_f1']:+.3f}")
    print("="*60 + "\n")
    
    # Save results
    with open(args.output, 'w') as f:
        json.dump(comparison, f, indent=2)
    logger.info(f"Results saved to {args.output}")

if __name__ == "__main__":
    main()

